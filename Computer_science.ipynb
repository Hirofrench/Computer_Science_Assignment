{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inleverbaar Computer_science_f1(0.30).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLE/FtGb1dY6CHqsrTthrq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hirofrench/Computer_Science_Assignment/blob/main/Computer_science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTmOtXUL1dxU"
      },
      "source": [
        "### Environment preperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lywy7Ej11U3",
        "outputId": "260cd01f-e10a-4743-e58c-fcafbf3738f5"
      },
      "source": [
        "# Code below is to make drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EpdohBJV3Kh"
      },
      "source": [
        "#install needed packages\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "!pip install python-Levenshtein\n",
        "!pip install searchgrid\n",
        "!pip install bayesian-optimization\n",
        "!pip install hyperopt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa0_6NV01MJo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "import time\n",
        "import warnings\n",
        "import Levenshtein as lev\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "from statistics import mean\n",
        "from math import *\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "from sklearn.utils import shuffle\n",
        "from random import choices, seed\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5YEcjii2MJ5"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYxpNbMJVLtV"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aemE47Qs2SUA",
        "outputId": "db47dcc8-8586-4cf9-80c7-a43162f56f2e"
      },
      "source": [
        "d = open('/content/drive/MyDrive/Com_assignment/TVs-all-merged.json')\n",
        "df = json.load(d)\n",
        "start = time.perf_counter()\n",
        "\n",
        "\n",
        "bestbuy = 0\n",
        "newegg = 0\n",
        "amazon = 0\n",
        "thenerds = 0\n",
        "count = 0\n",
        "\n",
        "common_brands = np.array([])\n",
        "shops = np.array([])\n",
        "df_clean = df\n",
        "\n",
        "title_remove_words = np.array([\"diag.\", \"[\", \"]\", \"open box:\", \"refurbished\", \"best buy\", \")\", \"(\", \",\"])\n",
        "\n",
        "for value in df_clean:\n",
        "  for tv in df_clean[value]:\n",
        "    #get vector with al shop names\n",
        "    if tv['shop'] not in shops:\n",
        "      shops = np.append(shops, tv['shop'])\n",
        "    \n",
        "    #clean title values\n",
        "    tv['title'] = tv['title'].lower()\n",
        "    tv['title'] = tv['title'].replace(\" hz\", \"hz\")\n",
        "    tv['title'] = tv['title'].replace(\"hertz\", \"hz\")\n",
        "    tv['title'] = tv['title'].replace(\"-hz\", \"hz\")\n",
        "    tv['title'] = tv['title'].replace(\" hertz\", \"hz\")\n",
        "\n",
        "    tv['title'] = tv['title'].replace(\"inches\", \"inch\")\n",
        "    tv['title'] = tv['title'].replace(\"-inch\", \"inch\")\n",
        "    tv['title'] = tv['title'].replace(\" inch\", \"inch\")\n",
        "    tv['title'] = tv['title'].replace(\"\\\"\", \"inch\")\n",
        "    tv['title'] = tv['title'].replace(\" inches\", \"inch\")\n",
        "\n",
        "    for word in title_remove_words:\n",
        "      tv['title'] = tv['title'].replace(word, \"\")\n",
        "    for word in shops:\n",
        "      tv['title'] = tv['title'].replace(word, \"\")\n",
        "\n",
        "    #as sometimes typos are made, i.e. we find 3112inch instead of 31-1/2 inch\n",
        "    for word in tv['title'].split():\n",
        "      if 'inch' in word and len(re.sub(\"[^\\d\\.]\", \"\", word))>3:\n",
        "        tv['title'] = tv['title'].replace(word, \"\")\n",
        "      if any(letter == \".\" for letter in word):\n",
        "        pos_dot = word.find(\".\")\n",
        "        #check if not a decimal number\n",
        "        if pos_dot != 0 and not word[pos_dot-1].isdigit():\n",
        "          tv['title'] = tv['title'].replace(word, \"\")\n",
        "    \n",
        "\n",
        "    #count tv's per shop\n",
        "    if 'bestbuy.com' in tv.values():\n",
        "      bestbuy+=1\n",
        "    elif 'newegg.com' in tv.values():\n",
        "      newegg+=1\n",
        "    elif 'amazon.com' in tv.values():\n",
        "      amazon+=1\n",
        "    elif 'thenerds.net' in tv.values():\n",
        "      thenerds+=1\n",
        "    count +=1\n",
        "    for feature in tv['featuresMap'].keys():\n",
        "      #clean feature valuies\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].lower()\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\" hz\", \"hz\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"-hz\", \"hz\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"hertz\", \"hz\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\" hertz\", \"hz\")\n",
        "\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"inches\", \"inch\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"-inch\", \"inch\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\" inch\", \"inch\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"\\\"\", \"inch\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\" inches\", \"inch\")\n",
        "      \n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"lbs\", \"lb\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"lbs.\", \"lb\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"lb.\", \"lb\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"pounds\", \"lb\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"pound\", \"lb\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\" lb\", \"lb\")\n",
        "\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\")\", \"\")\n",
        "      tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(\"(\", \"\")\n",
        "\n",
        "      #make decimal representation universal i.e. 7-1/2inch should be 7.5inch\n",
        "      feature_value = \"\"\n",
        "      for word in tv['featuresMap'][feature].split():\n",
        "        value = word\n",
        "        if \"http://\" in word:\n",
        "          tv['featuresMap'][feature] = tv['featuresMap'][feature].replace(word, \"\")\n",
        "        if any(letter==\"-\" for letter in word):\n",
        "          pos_bar = word.find(\"-\")\n",
        "          if any(letter==\"/\" for letter in word):\n",
        "            pos_slash = word.find(\"/\")\n",
        "            if len(word) > pos_bar and pos_slash>pos_bar:\n",
        "              \n",
        "              if word[pos_bar-1].isdigit() and word[pos_bar+1].isdigit():\n",
        "                string = \"\"\n",
        "                for i in range(len(word)):\n",
        "                  if word[i].isalpha() and not word[i].isdigit():\n",
        "                    string = string + word[i]\n",
        "                dec_number = re.sub(\"[^\\d\\.]\", \"\", word[pos_slash+1:])\n",
        "                dec_string = str(word[:pos_slash+1]+dec_number)\n",
        "                num, denom = dec_string[pos_bar+1:].split('/')\n",
        "\n",
        "                try:\n",
        "                  fraction = str(round(float(int(word[:pos_bar]) + float(int(num)/int(denom))),1))\n",
        "                except:\n",
        "                  num, denom = dec_string[pos_bar:].split('/')\n",
        "                  fraction = str(round(float(int(word[:pos_bar-1]) + float(int(num)/int(denom))),1))\n",
        "                value = str(fraction+string)\n",
        "              else:\n",
        "                ;\n",
        "            else:\n",
        "              ;\n",
        "          else:\n",
        "            ;\n",
        "        else:\n",
        "          \n",
        "          #Convert all decimal numbers to 1 decimal numbers to have a more universal set of measures\n",
        "          if any(letter==\".\" for letter in word):\n",
        "            pos_dot = word.find(\".\")\n",
        "            if pos_dot>0 and pos_dot<len(word)-2:\n",
        "              if word[pos_dot-1].isdigit() and word[pos_dot+1].isdigit() and word[pos_dot+2].isdigit():\n",
        "                string = \"\"\n",
        "                for i in range(len(word)):\n",
        "                  if word[i].isalpha() and not word[i].isdigit():\n",
        "                    string = string + word[i]\n",
        "                try:\n",
        "                  dec_number = re.sub(\"[^\\d\\.]\", \"\", word[pos_dot+1:])\n",
        "                  new_number = str(round(float(str(word[:pos_dot-1]+\".\"+dec_number)),1))\n",
        "                  value = str(new_number+string)\n",
        "                except:\n",
        "                  ; \n",
        "        if not feature_value:\n",
        "          feature_value = value\n",
        "        else:\n",
        "          feature_value = str(feature_value + \" \" + value)\n",
        "      tv['featuresMap'][feature] = feature_value\n",
        "      #print(feature_value)\n",
        "\n",
        "    #make uniform description for brand\n",
        "    if ('Brand' in tv['featuresMap'].keys() and tv['featuresMap']['Brand'] not in common_brands):\n",
        "      common_brands = np.append(common_brands, tv['featuresMap']['Brand'])\n",
        "    elif 'Brand Name' in tv['featuresMap'].keys():\n",
        "      tv['featuresMap']['Brand'] = tv['featuresMap'].pop('Brand Name')\n",
        "      if tv['featuresMap']['Brand'] not in common_brands:\n",
        "        common_brands = np.append(common_brands, tv['featuresMap']['Brand'])\n",
        "    elif 'Brand Name:' in tv['featuresMap'].keys():\n",
        "      tv['featuresMap']['Brand'] = tv['featuresMap'].pop('Brand Name:')\n",
        "      if tv['featuresMap']['Brand'] not in common_brands:\n",
        "        common_brands = np.append(common_brands, tv['featuresMap']['Brand'])\n",
        "common_brands = np.append(common_brands, [\"dynex\", \"insignia\"])\n",
        "#create universal screensize if possible\n",
        "search_key = 'Screen Size'\n",
        "for value in df_clean:\n",
        "  for tv in df_clean[value]:\n",
        "    res = dict(filter(lambda item: search_key in item[0], tv['featuresMap'].items()))\n",
        "\n",
        "    if res:\n",
        "      screen_size = list(res.values())[0]\n",
        "      screen_update = screen_size[0:6]\n",
        "      screen_size_update = re.sub(\"[^\\d\\.]\", \"\", screen_update)\n",
        "      screen_size_update = str(screen_size_update+\"inch\")\n",
        "      tv['featuresMap'].update({\"Universal Screensize\" : screen_size_update})\n",
        "    \n",
        "    elif 'inch' in tv['title']:\n",
        "      for word in tv['title'].split():\n",
        "        if 'inch' in word:\n",
        "          tv['featuresMap'].update({\"Universal Screensize\" : word}) \n",
        "    else:\n",
        "      tv['featuresMap'].update({\"Universal Screensize\" : \"not found\"})\n",
        "\n",
        "\n",
        "#some brand names are displayed in different names, check this\n",
        "wrong_brand = np.array([])\n",
        "good_brand = np.array([])\n",
        "for j in common_brands:\n",
        "  for k in common_brands:\n",
        "    if j in k and len(j)<len(k):\n",
        "      wrong_brand = np.append(wrong_brand, k)\n",
        "      good_brand = np.append(good_brand, j)\n",
        "\n",
        "#get missing brands from titlenames\n",
        "index = 0\n",
        "for value in df_clean:\n",
        "  for tv in df_clean[value]:  \n",
        "\n",
        "    tv.update({\"Index\": index})\n",
        "    index +=1\n",
        "    for j in common_brands:\n",
        "      if ('Brand' not in tv['featuresMap'].keys()):\n",
        "        if j in tv['title']:\n",
        "          tv['featuresMap'].update({\"Brand\": j})\n",
        "          break\n",
        "        elif common_brands[-1] == j:\n",
        "          tv['featuresMap'].update({\"Brand\": \"uncommon\"})\n",
        "    counter = -1\n",
        "    for k in wrong_brand:\n",
        "      counter+=1\n",
        "      if tv['featuresMap']['Brand'] == k:\n",
        "        tv['featuresMap']['Brand'] = good_brand[counter]\n",
        "        break\n",
        "end = time.perf_counter()\n",
        "print(\"Number of unique Tv's: \" + str(len(df)))\n",
        "print(\"Number of TV's sold by BestBuy: \" + str(bestbuy))\n",
        "print(\"Number of TV's sold by NewEgg: \" + str(newegg))\n",
        "print(\"Number of TV's sold by Amazon: \" + str(amazon))\n",
        "print(\"Number of TV's sold by TheNerds: \" + str(thenerds))\n",
        "print(\"Data size: \" + str(count))\n",
        "print(\"Number of duplicates: \" + str(count-len(df)))\n",
        "print(f\"Data transformation took {end - start:0.4f} seconds\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:145: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique Tv's: 1262\n",
            "Number of TV's sold by BestBuy: 773\n",
            "Number of TV's sold by NewEgg: 668\n",
            "Number of TV's sold by Amazon: 163\n",
            "Number of TV's sold by TheNerds: 20\n",
            "Data size: 1624\n",
            "Number of duplicates: 362\n",
            "Data transformation took 0.7656 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sShsf3sOthd"
      },
      "source": [
        "### Product representation: Model words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3JpyuYlO2VR"
      },
      "source": [
        "#This function extracts the modelwords from the data and creates the signaturematrix\n",
        "def modelwords_func(df_clean):\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "  global model_words\n",
        "  global model_words_title\n",
        "  global model_words_features\n",
        "  start = time.perf_counter()\n",
        "  model_words_title = np.array([])\n",
        "  model_words_features = np.array([])\n",
        "  must_contain_mw = ['hz', 'lbs', 'inch']\n",
        "  for value in df_clean:\n",
        "    for tv in df_clean[value]:\n",
        "      mw = tv['title'].split()\n",
        "      for word in mw:\n",
        "        if any(letter.isalpha() for letter in word):\n",
        "          if any(not letter.isalpha() for letter in word):\n",
        "            if word not in model_words_title:\n",
        "              model_words_title = np.append(model_words_title, word)\n",
        "        elif any(letter.isdigit() for letter in word):\n",
        "          if any(not letter.isdigit() for letter in word):\n",
        "            if word not in model_words_title:\n",
        "              model_words_title = np.append(model_words_title, word)\n",
        "      '''\n",
        "      #own representation\n",
        "      for features in tv['featuresMap'].values():\n",
        "        mw_features = features.split()\n",
        "        for i in must_contain_mw:\n",
        "          for word in mw_features:\n",
        "            if (i in word and word not in model_words_features):\n",
        "              if any(letter.isdigit() for letter in word):\n",
        "                model_words_features = np.append(model_words_features, word)\n",
        "      '''\n",
        "      for features in tv['featuresMap'].values():\n",
        "        mw_features = features.split()\n",
        "        for word in mw_features:\n",
        "          if any(letter == \".\" for letter in word):\n",
        "            if any(letter.isdigit() for letter in word):\n",
        "              if re.sub(\"[^\\d\\.]\", \"\", word) not in model_words_features:\n",
        "                model_words_features = np.append(model_words_features, re.sub(\"[^\\d\\.]\", \"\", word))\n",
        "\n",
        "  \n",
        "\n",
        "  print(\"   Amount of modelwords from title: \" + str(len(model_words_title)))\n",
        "  print(\"   Amount of modelwords from featuresmap: \" + str(len(model_words_features)))\n",
        "  model_words = np.concatenate((model_words_title, model_words_features))\n",
        "  #develop the signature matrix\n",
        "  df_update = df_clean\n",
        "  for value in df_update:\n",
        "    for tv in df_update[value]:\n",
        "      prod_rep_title = np.zeros(len(model_words_title), dtype=int)\n",
        "      prod_rep_features = np.zeros(len(model_words_features), dtype=int)\n",
        "      for j, mw in enumerate(model_words_title):\n",
        "        #if (mw in tv['title']):\n",
        "        if (mw in tv['title'] or mw in tv['featuresMap'].values()):\n",
        "          prod_rep_title[j] = 1\n",
        "        \n",
        "      \n",
        "      for k, mw in enumerate(model_words_features):\n",
        "        for j in tv['featuresMap'].values():\n",
        "          if mw in j:\n",
        "            prod_rep_features[k] = 1\n",
        "\n",
        "          \n",
        "      prod_representation = np.concatenate((prod_rep_title, prod_rep_features))\n",
        "      tv.update({\"Product Representation\" : prod_representation})\n",
        "  np.savetxt('model_words_test.csv', model_words, delimiter=',', fmt ='% s')\n",
        "  #np.savetxt('test.csv', binary_vector, delimiter=',', fmt ='% s')\n",
        "  end = time.perf_counter()\n",
        "  print(f\"   Extracting modelwords and constructing product representation took {end - start:0.4f} seconds\")\n",
        "  print(\"-\"*40)\n",
        "  return df_update, model_words\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw_EcJdVMoHq"
      },
      "source": [
        "### Min-Hashing and Locality Sensitivity Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaCLR5qDmBEe"
      },
      "source": [
        "#This function performs the minhashing\n",
        "def min_hash_func(df_clean, model_words, unique_keys):\n",
        "  #min-Hashing\n",
        "  start = time.perf_counter()\n",
        "  hashes = 200\n",
        "  signature_matrix = np.zeros((hashes,len(unique_keys)))\n",
        "  index = np.array([])\n",
        "\n",
        "  #create array of indices of model words\n",
        "  for i in range(len(model_words)):\n",
        "    index = np.append(index, i).astype(int)\n",
        "\n",
        "  #start minhashing  \n",
        "  for i in range(hashes):\n",
        "    #make permutation every hash\n",
        "    perm = shuffle(index, random_state = i)\n",
        "    product=0\n",
        "    for value in df_clean:\n",
        "      for tv in df_clean[value]:\n",
        "        counter=-1\n",
        "        for mw in range(len(model_words)):\n",
        "          counter+=1\n",
        "          permutated_index = perm[counter]\n",
        "          if tv['Product Representation'][permutated_index] == 1:\n",
        "            signature_matrix[i][product] = counter\n",
        "            product+=1\n",
        "            counter=-1\n",
        "            #if value of 1 is found, next product should be checked, therefore break\n",
        "            break\n",
        "  signature_matrix = signature_matrix.astype(int)\n",
        "  np.savetxt('sig_test.csv', signature_matrix, delimiter=',', fmt ='% s')\n",
        "  end = time.perf_counter()\n",
        "  print(f\"   Minhashing took {end - start:0.4f} seconds\")\n",
        "  print(\"-\"*40)     \n",
        "  return signature_matrix\n",
        "\n",
        "#This function performs the LSH\n",
        "def lsh_func(signature_matrix, considered_items):\n",
        "  #locality Sensitive Hashing (LSH)\n",
        "  start = time.perf_counter()\n",
        "  n, d = signature_matrix.shape\n",
        "  b = 60\n",
        "  r = n/b\n",
        "\n",
        "  #check if condition holds\n",
        "  assert n==b*r, \"    Something goes wrong within LSH\"\n",
        "  hashbuckets = defaultdict(set)\n",
        "  bands = np.array_split(signature_matrix, b, axis=0)\n",
        "  \n",
        "  for i, band in enumerate(bands):\n",
        "    for j in range(d):\n",
        "      band_id = tuple(list(band[:,j])+ [str(i)])\n",
        "      #print(band_id)\n",
        "      hashbuckets[band_id].add(j)\n",
        "  candidate_pairs = set()\n",
        "\n",
        "  for bucket in hashbuckets.values():\n",
        "    if len(bucket) >= 2:\n",
        "      possible_pairs = combinations(bucket,2)\n",
        "      for pair in possible_pairs:\n",
        "        candidate_pairs.add(pair)\n",
        "\n",
        "  #remove duplicates\n",
        "  dissim_matrix = np.full((d,d), -1*np.inf)\n",
        "  counter = 0\n",
        "  cand_pairs_no_duplicate = candidate_pairs\n",
        "  remove_set = set()\n",
        "  for i in cand_pairs_no_duplicate:\n",
        "    first = i[0]\n",
        "    second = i[1]\n",
        "    if dissim_matrix[first][second] == 1:\n",
        "      counter+=1\n",
        "      remove_set.add(i)\n",
        "    dissim_matrix[first][second] = 1\n",
        "    dissim_matrix[second][first] = 1\n",
        "\n",
        "  #remove duplicate sets\n",
        "  for i in remove_set:\n",
        "    cand_pairs_no_duplicate.remove(i)\n",
        "  end = time.perf_counter()\n",
        "  print(\"   \" + str(counter) + \" pairs have been removed as these were double in de candidate pairs\")\n",
        "  print(f\"   Locality Sensitivity Hashing took {end - start:0.4f} seconds\")\n",
        "  print(\"-\"*40)\n",
        "  return cand_pairs_no_duplicate"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewhEruXtkW1U"
      },
      "source": [
        "### Clustering Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZLEXSG0OlVv"
      },
      "source": [
        "#This function cleans the candidate pairs obtained from LSH based on certain product details\n",
        "def clean_candidate_pairs_func(candidate_pairs, df_update):\n",
        "  #remove candidate pairs from same shop and from different brands\n",
        "  start = time.perf_counter()\n",
        "  print(\"   Initial candidate pairs: \" + str(len(candidate_pairs)))\n",
        "  clean_cand_pairs = candidate_pairs\n",
        "  remove_set = set()\n",
        "  count_brand_removal = 0\n",
        "  count_screensize_removal = 0\n",
        "  count_keyvalue_removal = 0\n",
        "  for i in clean_cand_pairs:\n",
        "    screensize = np.array([])\n",
        "    brand = np.array([])\n",
        "    pair_index = np.array([i[0],i[1]])\n",
        "    pair_dict = dict()\n",
        "    tv_1 = dict()\n",
        "    tv_2 = dict()\n",
        "    key = 'first'\n",
        "    for j in pair_index:\n",
        "      for value in df_update:\n",
        "        for tv in df_update[value]:\n",
        "          if value == j:\n",
        "            pair_dict.update({key: tv}) \n",
        "\n",
        "            #find all key value pairs of which the value is either yes or no, or the value represents a digit of length 1\n",
        "            for key, feature in pair_dict['first']['featuresMap'].items():\n",
        "              if 'yes' in feature:\n",
        "                tv_1.update({key: 'yes'})\n",
        "              elif 'no' in feature:\n",
        "                tv_1.update({key: 'no'})\n",
        "              elif len(feature)==1 and feature.isdigit():\n",
        "                tv_1.update({key:feature })\n",
        "            if (len(pair_dict.keys())>1):\n",
        "              for key, feature in pair_dict['second']['featuresMap'].items():\n",
        "                if 'yes' in feature:\n",
        "                  tv_2.update({key: 'yes'})\n",
        "                elif 'no' in feature:\n",
        "                  tv_2.update({key: 'no'})\n",
        "                elif len(feature)==1 and feature.isdigit():\n",
        "                  tv_2.update({key:feature })\n",
        "            \n",
        "            screensize = np.append(screensize, re.sub(\"inch\", \"\", tv['featuresMap']['Universal Screensize']))\n",
        "            brand = np.append(brand, tv['featuresMap']['Brand'])\n",
        "            key = 'second'\n",
        "            break\n",
        "        else:\n",
        "          continue\n",
        "        break\n",
        "\n",
        "    #case 1: both brands are uncommon, therefor could be the same brand\n",
        "    if brand[0] == \"uncommon\" and brand[1] == \"uncommon\":\n",
        "      ;\n",
        "    #case 2: brands are not similar, therefor remove\n",
        "    elif brand[0] != brand[1]:\n",
        "      remove_set.add(i)\n",
        "      count_brand_removal+=1\n",
        "      # assert pair_dict['first']['featuresMap']['Brand'] != pair_dict['second']['featuresMap']['Brand'],  \\\n",
        "      # \"   Error: Two tv's of same brand have been removed.\"\n",
        "      # assert pair_dict['first']['modelID'] != pair_dict['second']['modelID'], \\\n",
        "      # \"   Error: Two same tv's have been removed, due to brand.\" \n",
        "\n",
        "    #remove pairs which do not have the same screensize. due to rounding, we accept maximum 0.5 difference\n",
        "    elif screensize[0] == \"not found\" or screensize[1] == \"not found\":\n",
        "      ;\n",
        "    elif abs(float(screensize[0]) - float(screensize[1])) >0.5:\n",
        "      remove_set.add(i)\n",
        "      count_screensize_removal+=1\n",
        "      # assert pair_dict['first']['modelID'] != pair_dict['second']['modelID'], \\\n",
        "      # \"   Error: Two same tv's have been removed, due to screensize.\"\n",
        "    elif bool(tv_1)==True and bool(tv_2)==True:\n",
        "      for key1, feature1 in tv_1.items(): \n",
        "        for key2, feature2 in tv_2.items(): \n",
        "          if key1==key2 and feature1!=feature2:\n",
        "            remove_set.add(i)\n",
        "            count_keyvalue_removal+=1\n",
        "\n",
        "  #remove all candidate pairs of which the shop is the same, under the assumption that one shop does not sell the same tv twice\n",
        "  for i in remove_set:\n",
        "    clean_cand_pairs.remove(i)\n",
        "  \n",
        "  end = time.perf_counter() \n",
        "\n",
        "  print(\"   Removal different brand: \" + str(count_brand_removal))\n",
        "  print(\"   Removal due to different screensize: \" + str(count_screensize_removal))\n",
        "  print(\"   Removal due to different key-value pairs: \" + str(count_keyvalue_removal))\n",
        "  print(\"   New number of candidate pairs: \" + str(len(clean_cand_pairs)))\n",
        "  print(f\"   Pair cleaning took {end - start:0.4f} seconds\")\n",
        "  print(\"-\"*40)\n",
        "  return clean_cand_pairs\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uOByiiJmtLT"
      },
      "source": [
        "def tdif ():\n",
        "  dissim_matrix_update = dissim_matrix\n",
        "  for i in range(len(dissim_matrix)):\n",
        "    for j in range(len(dissim_matrix)):\n",
        "      if dissim_matrix[i][j] == 1:\n",
        "        titles = [df_update[i][0]['title'],df_update[j][0]['title']]\n",
        "        vect = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
        "        tfidf = vect.fit_transform(titles) \n",
        "        pairwise_similarity = tfidf * tfidf.T \n",
        "        pairwise_similarity.toarray() \n",
        "        dissim_matrix_update[i][j] = pairwise_similarity.toarray()[0][1]\n",
        "  correct = 0\n",
        "  not_correct = 0\n",
        "  for i in range(len(dissim_matrix_update)):\n",
        "    for j in range(len(dissim_matrix_update)):\n",
        "      if dissim_matrix[i][j]>0.2 and i<j:\n",
        "        if df_update[i][0]['modelID'] == df_update[j][0]['modelID']:\n",
        "          correct+=1\n",
        "        else:\n",
        "          not_correct+=1\n",
        "  print(correct)\n",
        "  print(not_correct)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP2AzN3dHgSQ"
      },
      "source": [
        "#This function calculates the dice coefficient\n",
        "def dice_coef(set1, set2):\n",
        "  count = 0\n",
        "  for i in set1:\n",
        "    if i in set2:\n",
        "      count+=1    \n",
        "  dice =  2*count/(len(set1)+len(set2))\n",
        "  return dice\n",
        "\n",
        "#This function creates all the n-grams and eventually finds the final pairs based on the similarities between the n-grams\n",
        "def ngram_func(cand_pairs, df_update, shops, common_brands, alpha, beta, theta, gamma,mu,unique_keys):\n",
        "  cleaned_pairs = cand_pairs \n",
        "  dissim_matrix = np.full((len(df_update),len(df_update)), -1*np.inf)\n",
        "  dissim_kv = dissim_matrix\n",
        "  \n",
        "  #these words should be removed from the title as they are meaningless or already filtered on\n",
        "  rem_words = np.append(common_brands, [\"inch\", \"best buy\", \"Diag.\", \"amazon\"]) \n",
        "  remove_words = np.append(rem_words, shops)\n",
        "  for i in cleaned_pairs:\n",
        "    first_tv = df_update[i[0]][0]\n",
        "    second_tv = df_update[i[1]][0] \n",
        "    tv_value1 = dict()\n",
        "    tv_value2 = dict()\n",
        "    tv_key1 = dict()\n",
        "    tv_key2 = dict()\n",
        "    first = i[0]\n",
        "    second = i[1]\n",
        "    \n",
        "    #n grams for key value pairs\n",
        "    for key, feature in first_tv['featuresMap'].items():\n",
        "      n_grams_kv_1 = []\n",
        "      ngram_key1 = []\n",
        "      first_tv['featuresMap'][key] = first_tv['featuresMap'][key].replace(\",\", \"\")\n",
        "      first_tv['featuresMap'][key] = first_tv['featuresMap'][key].replace(\"/\", \"\")\n",
        "      first_tv['featuresMap'][key] = first_tv['featuresMap'][key].replace(\"\\ \", \"\")\n",
        "      for word in feature.split():\n",
        "        for j in range(len(word)-2): \n",
        "          if len(word)>=3:   \n",
        "              n_grams_kv_1.append(word[j: j + 3])\n",
        "\n",
        "      #make ngrams from keys\n",
        "      for word in key.split():\n",
        "        for j in range(len(word)-2):\n",
        "          if len(word)>=3:\n",
        "            ngram_key1.append(word[j: j + 3])\n",
        "\n",
        "      #exclude information about brand, screensize etc. as these are already filtered on and will shift similarity towards each other\n",
        "      if key != 'Brand' and 'inch' not in feature and len(n_grams_kv_1)>0 and feature != 'yes' and feature != 'no':\n",
        "        tv_value1.update({key: n_grams_kv_1})\n",
        "        tv_key1.update({key:ngram_key1})\n",
        "     \n",
        "    for key, feature in second_tv['featuresMap'].items():\n",
        "      n_grams_kv_2 = []\n",
        "      ngram_key2 = []\n",
        "      second_tv['featuresMap'][key] = second_tv['featuresMap'][key].replace(\",\", \"\")\n",
        "      second_tv['featuresMap'][key] = second_tv['featuresMap'][key].replace(\"/\", \"\")\n",
        "      second_tv['featuresMap'][key] = second_tv['featuresMap'][key].replace(\"\\ \", \"\")\n",
        "      for word in feature.split():\n",
        "        for j in range(len(word)-2): \n",
        "          if len(word)>=3:\n",
        "            n_grams_kv_2.append(word[j: j + 3])\n",
        "\n",
        "      #make ngrams from keys\n",
        "      for word in key.split():\n",
        "        for j in range(len(word)-2):\n",
        "          if len(word)>=3:\n",
        "            ngram_key2.append(word[j: j + 3])\n",
        "\n",
        "      if key != 'Brand' and 'inch' not in feature and len(n_grams_kv_2)>0 and feature != 'yes' and feature != 'no':\n",
        "        tv_value2.update({key: n_grams_kv_2})\n",
        "        tv_key2.update({key:ngram_key2})\n",
        "\n",
        "    dice_kv = []\n",
        "    match = dict()\n",
        "    match_dice = dict()\n",
        "    for key, ngram in tv_key1.items():\n",
        "      for key2, ngram2 in tv_key2.items():\n",
        "        dice_key = dice_coef(ngram, ngram2)\n",
        "        #check for similarity in keys\n",
        "        if dice_key >= beta:\n",
        "          #create dictionary with matching keys which have a dice coefficient of minimum beta\n",
        "          match.update({key:key2})\n",
        "          match_dice.update({key:dice_key})\n",
        "    weight = 0\n",
        "    #find dice coefficient for values of 'similar' keys\n",
        "    for key, ngram in tv_value1.items():    \n",
        "      for match1, match2 in match.items():      \n",
        "        if key==match1:\n",
        "          ngram2 = tv_value2[match2]\n",
        "          dice_it = dice_coef (ngram, ngram2)\n",
        "          dice_key = match_dice[match1]\n",
        "          weight = weight+dice_key\n",
        "          dice_kv.append(dice_it*dice_key)\n",
        "    if len(dice_kv)>0:\n",
        "      dissim_kv[first][second] = sum(dice_kv)/weight\n",
        "      dissim_kv[second][first] = sum(dice_kv)/weight\n",
        "      #print(dissim_kv[first][second])\n",
        "    #if no matching keys assign 0\n",
        "    else:\n",
        "      dissim_kv[first][second] = 0\n",
        "      dissim_kv[second][first] = 0\n",
        "\n",
        "    \n",
        "  \n",
        "  dissim_matrix_update = np.full((len(df_update),len(df_update)), -1*np.inf)\n",
        "  #n grams for title\n",
        "  dissim_tmwm = np.full((len(df_update),len(df_update)), -1*np.inf)\n",
        "  for i in cleaned_pairs:\n",
        "    first = i[0]\n",
        "    second = i[1]\n",
        "    first_tv = df_update[i[0]][0]\n",
        "    second_tv = df_update[i[1]][0] \n",
        "    title_1 = first_tv['title']\n",
        "    title_2 = second_tv['title']\n",
        "    for word in remove_words:\n",
        "      for title_word in title_1.split():\n",
        "        title_1 = title_1.replace(\",\", \"\")\n",
        "        title_1 = title_1.replace(\"/\", \"\")\n",
        "        title_1 = title_1.replace(\"\\ \", \"\")\n",
        "        title_1 = title_1.replace(\":\", \"\")\n",
        "        if word in title_word or len(title_word)==1: \n",
        "          title_1 = title_1.replace(title_word, \"\")\n",
        "      for title_word in title_1.split():\n",
        "        if word in title_word or len(title_word)==1: \n",
        "          title_2 = title_2.replace(title_word, \"\")\n",
        "    n_grams_1 = []\n",
        "    n_grams_2 = []\n",
        "    for word in title_1.split():\n",
        "      for i in range(len(word)-2): \n",
        "        n_grams_1.append(word[i: i + 3])\n",
        "    for word in title_2.split():\n",
        "      for i in range(len(word)-2): \n",
        "        n_grams_2.append(word[i: i + 3])\n",
        "    \n",
        "    dice_sim = dice_coef(n_grams_1, n_grams_2)\n",
        "    dissim_matrix_update[first][second] = dice_sim\n",
        "    dissim_matrix_update[second][first] = dice_sim\n",
        "\n",
        "    #calculate tmwm similarity\n",
        "    tmwm_sim = tmwm_func(title_1, title_2, df_update, gamma, first_tv, second_tv)\n",
        "    dissim_tmwm[first][second] = tmwm_sim\n",
        "    dissim_tmwm[second][first] = tmwm_sim\n",
        "\n",
        "\n",
        "  #make a dissimilarity matrix based on key value pair and title\n",
        "  dissim_mix = np.full((len(df_update),len(df_update)), -1*np.inf)\n",
        "  for i in range(0, len(dissim_mix)-1):\n",
        "    for j in range(i+1, len(dissim_mix)):\n",
        "      if dissim_kv[i][j]==0:\n",
        "        dissim_mix[i][j] = dissim_matrix_update[i][j]\n",
        "        dissim_mix[j][i] = dissim_matrix_update[i][j]\n",
        "      if dissim_kv[i][j]>=0:\n",
        "        average_value = alpha*dissim_matrix_update[i][j] + mu*dissim_kv[i][j] + (1-alpha-mu)*dissim_tmwm[i][j]\n",
        "        #average_value = alpha*dissim_kv[i][j] + (1-alpha)*dissim_tmwm[i][j]\n",
        "        dissim_mix[i][j] = average_value\n",
        "        dissim_mix[j][i] = average_value\n",
        "\n",
        "  correct =0\n",
        "  not_correct=0\n",
        "\n",
        "  #return final set of candidate pairs\n",
        "  remove_set = set()\n",
        "  \n",
        "  for k in cleaned_pairs:\n",
        "    i = k[0]\n",
        "    j = k[1]\n",
        "    if dissim_mix[i][j]>=theta:\n",
        "      if df_update[i][0]['modelID'] == df_update[j][0]['modelID']:\n",
        "        correct+=1\n",
        "      elif dissim_mix[i][j]>=0:\n",
        "        not_correct+=1\n",
        "    else:\n",
        "      remove_set.add(k)\n",
        "  clean_pairs = set()\n",
        "  for i in cleaned_pairs:\n",
        "    clean_pairs.add(i)\n",
        "  for i in remove_set:\n",
        "    clean_pairs.remove(i)\n",
        "  return clean_pairs, dissim_kv, dissim_matrix_update\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG0U12UzFkyA"
      },
      "source": [
        "#def tmwm_func(cand_pairs):\n",
        "\n",
        "\n",
        "\n",
        "def get_cosine(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = sqrt(sum1) * sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def text_to_vector(text):\n",
        "    word = re.compile(r'\\w+')\n",
        "    words = word.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "def tmwm_func(first_title, second_title, df_update, gamma, first_tv, second_tv):\n",
        "  title_1 = text_to_vector(first_title)\n",
        "  title_2 = text_to_vector(second_title)\n",
        "  cos_sim = get_cosine(title_1, title_2)\n",
        "  if cos_sim < gamma:\n",
        "    total_sim = 0\n",
        "  else:\n",
        "    total_sim = cos_sim\n",
        "  mw_2 = np.array([])\n",
        "  mw_1 = np.array([]) \n",
        "  for word in model_words_title:\n",
        "    if word in first_tv['title']:\n",
        "      mw_1 = np.append(mw_1, word)\n",
        "    if word in second_tv['title']:\n",
        "      mw_2 = np.append(mw_2, word)\n",
        "  for j in mw_1:\n",
        "    for k in mw_2:\n",
        "      if any(letter.isdigit() for letter in j) and any(letter.isdigit() for letter in k) and \\\n",
        "      any(letter.isalpha() for letter in j) and any(letter.isalpha() for letter in k):\n",
        "        non_numeric_1 = re.sub(\" \\d+\", \" \", j) \n",
        "        non_numeric_2 = re.sub(\" \\d+\", \" \", k)\n",
        "        bigram_1 = []\n",
        "        bigram_2 = []\n",
        "        for t in range(len(non_numeric_1)-1):\n",
        "          if len(non_numeric_1)>=2:\n",
        "            bigram_1.append(non_numeric_1[t: t + 2])\n",
        "        for t in range(len(non_numeric_2)-1):\n",
        "          if len(non_numeric_2)>=2:\n",
        "            bigram_2.append(non_numeric_2[t: t + 2])\n",
        "        if bigram_1 and bigram_2:\n",
        "          dice = dice_coef(bigram_1, bigram_2)\n",
        "        else:\n",
        "          dice = 0\n",
        "        if dice>0.9:\n",
        "          numeric_1 = re.findall('[0-9]+', j)\n",
        "          numeric_2 = re.findall('[0-9]+', k)\n",
        "          if numeric_1 != numeric_2:\n",
        "            total_sim = 0\n",
        "            break\n",
        "          \n",
        "  return total_sim\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB1zc8Ba5XTx"
      },
      "source": [
        "def split_pairs(candidate_pairs,nr_groups):\n",
        "  nr_pgroup = floor(len(candidate_pairs)/nr_groups)\n",
        "  rest = len(candidate_pairs) - nr_pgroup*nr_groups\n",
        "  new_groups = dict()\n",
        "  counter = 1\n",
        "  pairs = set()\n",
        "  for j, pair in enumerate(candidate_pairs):\n",
        "    pairs.add(pair)\n",
        "    if j== counter*nr_pgroup:\n",
        "      new_groups.update({counter:pairs})\n",
        "      counter+=1\n",
        "      pairs= set()\n",
        "    elif j==len(candidate_pairs):\n",
        "      new_groups.update({counter:pairs})\n",
        "  return new_groups"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the parameter grids of the bayesian optimization\n",
        "#set startvalue, so no errors are given\n",
        "alpha = 0\n",
        "bayes_parameters = {'alpha' : hp.uniform('alpha',0,1),\n",
        "                    'beta' : hp.uniform('beta',0,1),\n",
        "                    'theta' : hp.uniform('theta',0,1),\n",
        "                    'gamma' : hp.uniform('gamma',0,1),\n",
        "                    'mu' : hp.uniform('mu',0,1-alpha)}"
      ],
      "metadata": {
        "id": "sVANN5NP-az7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function should be minimized by the bayesian optimization, is minimizes the loss function, which is -1*f1-value\n",
        "def bayesgrid_search(space):\n",
        "  alpha = space['alpha']\n",
        "  beta = space['beta']\n",
        "  theta = space['theta']\n",
        "  gamma = space['gamma']\n",
        "  mu = space['mu']\n",
        "  cand_pairs = pairs_bayes\n",
        "  total_pairs = 0\n",
        "  for t in cand_pairs:\n",
        "    first = t[0]\n",
        "    second = t[1]\n",
        "    if bayes_df[first][0]['modelID'] == bayes_df[second][0]['modelID']:\n",
        "      total_pairs+=1\n",
        "  pairs_update = ngram_func(cand_pairs, bayes_df, shops, common_brands, alpha, beta, theta, gamma,mu,unique_keys)\n",
        "  f1_it, p_quality, p_complete = f1_func(pairs_update[0], bayes_df, graph_total_pairs, unique_keys)\n",
        "\n",
        "  return {'loss': -f1_it, 'pq': p_quality, 'pc': p_complete, 'status': STATUS_OK}"
      ],
      "metadata": {
        "id": "jiWp9-WR9p4D"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td1RkN-ZYBxE"
      },
      "source": [
        "#This function runs the whole algorithm and returns either all the parameters if it's the train set, otherwise the f1 value if it's the test set\n",
        "\n",
        "def algorithm_func(df_alg, unique_keys, shops, common_brands, train_test,alpha,beta,theta,gamma,mu,i):\n",
        "  it_start = time.perf_counter()\n",
        "  global graph_total_pairs\n",
        "  #check possible pairs for this bootstrap    \n",
        "  print(\"   Amount of tv's in bootstrap \" + str(i+1) + \": \" + str(len(unique_keys)))\n",
        "  modelID_pairs, total_pairs = duplicate_count(unique_keys, df_alg)\n",
        "  graph_total_pairs = total_pairs\n",
        "  #make variables global, such that they can be found during bayesian optimization\n",
        "  global pairs_bayes\n",
        "  global bayes_df\n",
        "\n",
        "  #get modelwords from current sample\n",
        "  bayes_df, model_words = modelwords_func(df_alg)\n",
        "  #get signature matrix with minhashing\n",
        "  signature_matrix = min_hash_func(bayes_df, model_words, unique_keys)\n",
        "  #get candidate pairs with lsh\n",
        "  candidate_pairs = lsh_func(signature_matrix, bayes_df)\n",
        "  fraction_comparisons = len(candidate_pairs)/nCr(len(unique_keys),2)\n",
        "  #get f1 measure pure on LSH\n",
        "  f1_lsh, pq_lsh, pc_lsh = f1_func(candidate_pairs, bayes_df, total_pairs, unique_keys)\n",
        "  #clean candidate pairs\n",
        "  pairs_bayes = clean_candidate_pairs_func(candidate_pairs, bayes_df)\n",
        "\n",
        "  #clustering with grid search\n",
        "  if train_test == 'train':\n",
        "    trials = Trials()\n",
        "    best_params = fmin(fn=bayesgrid_search,\n",
        "            space=bayes_parameters,\n",
        "            algo=tpe.suggest,\n",
        "            trials=trials,\n",
        "            return_argmin=False, # Return categorical vars as strings instead of indices\n",
        "            max_evals=20,\n",
        "            rstate= np.random.RandomState(i))\n",
        "    best_alpha = best_params['alpha']\n",
        "    best_beta = best_params['beta']\n",
        "    best_theta = best_params['theta']\n",
        "    best_gamma = best_params['gamma']\n",
        "    best_mu = best_params['mu'] \n",
        "    best_f1  = [x['result']['loss'] for x in trials.trials]\n",
        "    index_min = best_f1.index(min(best_f1))\n",
        "    best_pq  = [x['result']['pq'] for x in trials.trials][index_min]\n",
        "    best_pc  = [x['result']['pc'] for x in trials.trials][index_min]\n",
        "  elif train_test == 'test':\n",
        "    final_pairs,dissim_kv, dissim_matrix_update = ngram_func(pairs_bayes, bayes_df, shops, common_brands, alpha, beta, theta, gamma,mu, unique_keys )\n",
        "    f1_iteration, p_quality, p_complete = f1_func(final_pairs, bayes_df, total_pairs, unique_keys)\n",
        "  \n",
        "  it_end = time.perf_counter()\n",
        "  print(f\"   Iteration took {it_end - it_start:0.4f} seconds\")\n",
        "  print(\"*\"*60)\n",
        "  if train_test == 'train':\n",
        "    return best_alpha, best_beta, best_theta, best_gamma, best_mu, best_f1, f1_lsh, pq_lsh, pc_lsh\n",
        "    #return best_alpha, best_beta, best_theta, best_gamma, best_mu,  best_f1, f1_lsh, pq_lsh, pc_lsh\n",
        "  else:\n",
        "    return f1_iteration, p_quality, p_complete"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx39uI27J5y7"
      },
      "source": [
        "### Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An-tEWC4iNTA"
      },
      "source": [
        "#This is a simple function which calculates nCr\n",
        "def nCr(n,r):\n",
        "  f = factorial\n",
        "  return f(n) / (f(r) * f(n-r))\n",
        "\n",
        "#This function counts per dataset howmuch possible pairs there are in the set\n",
        "def duplicate_count(unique_keys, df_bootstrap):\n",
        "  unique_items = unique_keys.astype(int)\n",
        "  modelID_pairs = dict()\n",
        "  model_test = np.array([])\n",
        "  \n",
        "  for value in df_bootstrap:\n",
        "    for tv in df_bootstrap[value]:\n",
        "      model_test = np.append(model_test, tv['modelID'])\n",
        "  model_unique = np.unique(model_test)\n",
        "\n",
        "  total_pairs = 0\n",
        "  for i in model_unique:\n",
        "    counter = 0\n",
        "    for value in df_bootstrap:\n",
        "      for tv in df_bootstrap[value]:\n",
        "        if i == tv['modelID']:\n",
        "          counter+=1\n",
        "\n",
        "    if counter>=2:\n",
        "      possible_pairs = nCr(counter,2)\n",
        "    else:\n",
        "      possible_pairs = 0\n",
        "    total_pairs = total_pairs + possible_pairs\n",
        "    assert counter>0, \"   Duplicate count mistake\"\n",
        "    modelID_pairs.update({i: possible_pairs})\n",
        "\n",
        "  print(\"   Amount of pairs: \" + str(int(total_pairs)))\n",
        "  print(\"-\"*40)\n",
        "  return modelID_pairs, total_pairs\n",
        "\n",
        "def f1_func(cleaned_pairs, df_f1, total_pairs, unique_keys):\n",
        "  correct = 0\n",
        "  not_correct = 0\n",
        "  f1_measure = 0\n",
        "  pair_quality = 0\n",
        "  pair_completeness = 0\n",
        "  for i in cleaned_pairs:\n",
        "    pair_index = np.array([i[0],i[1]])\n",
        "    pair_model = np.array([])\n",
        "    \n",
        "    for j in pair_index:\n",
        "      for value in df_f1:\n",
        "        for tv in df_f1[value]:\n",
        "          if value == j:\n",
        "            pair_model = np.append(pair_model, tv['modelID'])\n",
        "            break\n",
        "        else:\n",
        "          continue\n",
        "        break\n",
        "    if pair_model[0] == pair_model[1]:\n",
        "      correct+=1\n",
        "      assert df_f1[i[0]][0]['modelID'] == df_f1[i[1]][0]['modelID'], \\\n",
        "      \"   Mistake in classifying correct pairs\"\n",
        "    else:\n",
        "      not_correct+=1\n",
        "      assert df_f1[i[0]][0]['modelID'] != df_f1[i[1]][0]['modelID'], \\\n",
        "      \"   Mistake in classifying correct pairs\"\n",
        "  if correct>0 and not_correct>0:\n",
        "    pair_quality = correct/(correct+not_correct)\n",
        "    pair_completeness = correct / total_pairs\n",
        "    fraction_comparisons = len(cleaned_pairs) / nCr(len(unique_keys),2)\n",
        "    f1_measure = (2*pair_quality*pair_completeness)/(pair_quality + pair_completeness)\n",
        "    f1_alt = correct/(correct + 0.5*(not_correct+(total_pairs-correct)))\n",
        "    print(\"   Correct pair: \" + str(correct))\n",
        "    print(\"   Not correct pair: \" + str(not_correct))\n",
        "    # print(\"   Pair quality: \" + str(pair_quality))\n",
        "    # print(\"   Pair completeness: \" + str(pair_completeness))\n",
        "    # print(\"   Fraction of comparisons: \" + str(fraction_comparisons))\n",
        "    # print(\"   F1 value: \" + str(f1_measure))\n",
        "    # print(\"   F1 alternative value: \" + str(f1_alt))\n",
        "  else:\n",
        "    print(\"   No comparisons are made.\")\n",
        "  print(\"-\"*40)\n",
        "  return f1_measure, pair_quality, pair_completeness\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6rmxD-YVdQA"
      },
      "source": [
        "### Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg7tK4b3KBZw"
      },
      "source": [
        "nr_bootstraps = 20\n",
        "index_dict = np.array([])\n",
        "f1_measure = np.array([])\n",
        "pc_measure = np.array([])\n",
        "pq_measure = np.array([])\n",
        "alpha_overal = dict()\n",
        "beta_overal = dict()\n",
        "theta_overal = dict()\n",
        "mu_overal = dict()\n",
        "gamma_overal = dict()\n",
        "f1_overal = dict()\n",
        "f1_lsh = dict()\n",
        "pq_lsh = dict()\n",
        "pc_lsh = dict()\n",
        "#create array of indices of model words\n",
        "for i in range(count):\n",
        "  index_dict = np.append(index_dict, i).astype(int)\n",
        "\n",
        "for i in range(nr_bootstraps):\n",
        "  total_start = time.perf_counter()\n",
        "  print (\"Iteration: \" + str(i+1))\n",
        "  df_train = dict()\n",
        "  df_test = dict()\n",
        "  np.random.seed(i)\n",
        "  global unique_keys\n",
        "  keys = np.random.choice(a=index_dict,replace = True, size=len(index_dict)*1)\n",
        "  unique_keys = np.unique(keys)\n",
        "  dissim_matrix = np.zeros((len(unique_keys), len(unique_keys)))\n",
        "  test_key = np.array([])\n",
        "  #find values for test set\n",
        "  for value in df_clean:\n",
        "    for tv in df_clean[value]: \n",
        "      if tv['Index'] not in unique_keys:\n",
        "        test_key = np.append(test_key, int(tv['Index']))\n",
        "  test_key = test_key.astype(int)\n",
        "\n",
        "  #make dictionary with test set\n",
        "  for index, j in enumerate(test_key):\n",
        "    for value in df_clean:\n",
        "      for tv in df_clean[value]:\n",
        "        if tv['Index'] == j:\n",
        "          df_test.update({index:[tv]})\n",
        "          break\n",
        "      else:\n",
        "        continue\n",
        "      break\n",
        "\n",
        "  for index, j in enumerate(unique_keys):\n",
        "    for value in df_clean:\n",
        "      for tv in df_clean[value]:\n",
        "        if tv['Index'] == j:\n",
        "          df_train.update({index:[tv]})\n",
        "          break\n",
        "      else:\n",
        "        continue\n",
        "      break\n",
        "  \n",
        "\n",
        "  #obtain best parameters in train set\n",
        "  alpha, beta, theta, gamma, mu,  f1_train, f1_hash, pq_hash, pc_hash = algorithm_func(df_train, unique_keys, shops, common_brands, 'train', 0,0,0,0,0,i)\n",
        "\n",
        "  #save all values obtainend in dictionary\n",
        "  f1_max = -1*min(f1_train)\n",
        "  alpha_overal.update({i:alpha})\n",
        "  beta_overal.update({i:beta})\n",
        "  theta_overal.update({i:theta})\n",
        "  mu_overal.update({i:mu})\n",
        "  gamma_overal.update({i:gamma})\n",
        "  f1_overal.update({i:f1_max})\n",
        "  f1_lsh.update({i:f1_hash})\n",
        "  pc_lsh.update({i:pc_hash})\n",
        "  pq_lsh.update({i:pq_hash})\n",
        "  print(\"   TRAIN OUTPUT\")\n",
        "  print(alpha_overal)\n",
        "  print(beta_overal)\n",
        "  print(theta_overal)\n",
        "  print(gamma_overal)\n",
        "  print(mu_overal)\n",
        "  print(f1_overal)\n",
        "  print(\"*\"*60)\n",
        "  \n",
        "  unique_keys = test_key\n",
        "  #get results for test set\n",
        "  f1_test, pq_test, pc_test = algorithm_func(df_test, unique_keys, shops, common_brands, 'test', alpha, beta, theta,gamma,mu,i)\n",
        "\n",
        "  print(\"   TEST OUTPUT\")\n",
        "  # #evaluate the models \n",
        "  f1_measure = np.append(f1_measure, f1_test)\n",
        "  pc_measure = np.append(pc_measure, pc_test)\n",
        "  pq_measure = np.append(pq_measure, pq_test)\n",
        "  print(f1_measure)\n",
        "  print(pc_measure)\n",
        "  print(pq_measure)\n",
        "\n",
        "  total_end = time.perf_counter()\n",
        "  print(f\"   Iteration took {total_end - total_start:0.4f} seconds\")\n",
        "  print(\"*\"*60)\n",
        "print(\"Average F1 value: \" +  str(mean(f1_measure)))\n",
        "print(\"Average Pair Quality: \" + str(mean(pq_measure)))\n",
        "print(\"Average Pair Completeness: \" + str(mean(pc_measure)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}